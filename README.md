# galvASR: Daniel __Galv__ez's __A__utomatic __S__peech __R__ecognition

## Warning

This library is under active development. There will definitely be bugs.

## Motivation

I am one of the maintainers of the Kaldi Speech Recognition system. It
is, in a lot of people's opinions, probably the best free open source
speech recognition system. Its style of speech recognition models,
hybrid HMM-DNNs, are at the time of writing the state-of-the-art in
speech recognition, thanks to innovations like time-delay neural
networks and lattice-free MMI. No other software library supports as
many datasets as it does out-of-the-box via its recipe ("egs")
scripts. One of these libraries in particular, Librispeech, was
developed by the team as the first large (~1000+ hours) free speech
recognition corpus. I know a few too many speech startups that got
their start based solely on this dataset!

Lately, we've seen a rise in deep learning frameworks, and with that
an interest in "end-to-end" or fully-neural speech recognition systems
lately.

Basically all of these frameworks have chosen a python-based
interface, with a C++ core. This is a marked step away from Kaldi's
core of C++ executables that are driven by bash scripts. In my
interactions with people in industry, it's become clear that many
people would like a more familiar Python interface, while still having
many of the advantages of Kaldi.

Therefore, galvASR provides tensorflow datasets ops caffe2 ops to read
Kaldi's flexible key-value pair [table
files](http://kaldi-asr.org/doc/io_tut.html). This means you can take
any [Kaldi-prepared data
directory](http://kaldi-asr.org/doc/data_prep.html) generated by one
of Kaldi's existing recipes and do training on it immediately with
your framework of choice.

I've also noticed that there has been a lot of fragementation recently
in terms of making work reproducible in the field of speech
recognition, with many research groups seeking improvement via very
large datasets. However, none of these groups has studied whether
other group's models get the same benefits from larger datasets. I
would like to the hypothesis that

Therefore, another goal of galvASR is to make a large, multidataset
benchmarks, one using only free data (we can get ~2000 hours this way,
the size of the common Switchboard-Fisher benchmark), and another
combining free and non-free datasets (I'd like to reach 10,000 hours).

## Install

See the Dockerfile to get the dependencies.

Note that you will need nvidia-docker to run this because of the CUDA
dependency.

```
cd scripts/docker
docker build -t galv-asr DefaultBuild
```

## Build

This library uses submodules! Read more about how to handle those [here](https://stackoverflow.com/a/1032653/3469721)

```
git clone --recursive https://github.com/galv/galvASR.git
cd galvASR &&
mkdir build &&
cd build &&
cmake -DPYTHON_INSTALL_EDITABLE=NO -DCMAKE_INSTALL_PREFIX=../install -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DUSE_CAFFE2=NO -DUSE_TENSORFLOW=YES ..
make -k -j27
```

## Run
